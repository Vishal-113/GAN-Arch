{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMM6WdeatFySohfFhNsLw2a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishal-113/GAN-Arch/blob/main/ETHICS_and_AI_harm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chosen AI Harm: Representational Harm**\n",
        "Representational harm occurs when AI systems reinforce stereotypes or misrepresent certain groups, leading to unfair portrayals and societal biases.\n",
        "\n",
        "\n",
        "\n",
        "### **Hypothetical Application: AI-Powered Hiring Assistant**\n",
        "Imagine a company using an AI-powered hiring assistant to screen resumes and rank candidates. If the AI model is trained on biased historical hiring data, it may:\n",
        "- **Underrepresent certain groups** by favoring resumes that match past hiring patterns.\n",
        "- **Reinforce stereotypes** by associating certain job roles with specific demographics.\n",
        "\n",
        "For example, if the AI has learned that past successful software engineers were predominantly male, it might rank female candidates lower, even if they are equally qualified.\n",
        "\n",
        "\n",
        "\n",
        "### **Harm Mitigation Strategies**\n",
        "1. **Bias Auditing and Diverse Training Data**\n",
        "   - Regularly audit the AI model for biases in its decision-making.\n",
        "   - Train the model on diverse datasets that include balanced representations of different demographics.\n",
        "\n",
        "2. **Human Oversight and Fairness Constraints**\n",
        "   - Implement human review processes to ensure AI-generated rankings do not unfairly disadvantage certain groups.\n",
        "   - Apply fairness constraints in the AI model to prevent discriminatory patterns from influencing hiring decisions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ohoF-RrD8HG3"
      }
    }
  ]
}